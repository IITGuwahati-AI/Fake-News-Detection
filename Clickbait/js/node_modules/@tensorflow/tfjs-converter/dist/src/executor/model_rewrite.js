"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
var tfjs_core_1 = require("@tensorflow/tfjs-core");
/**
 * This graph rewrite rule tries to identify the PRelu structure generated by
 * tf.keras, and convert it to tfjs core prelu op.
 *
 * The formula of PReLU is:
 * f(x) = alpha * x for x < 0, f(x) = x for x >= 0.
 *
 * `x` is the input, and `alpha` is a trainable tensor which can be broadcasted
 * to the shape of `x`.
 *
 * There's no native PRelu op in TensorFlow, so tf.keras generates the following
 * structure which does the equivalent calculation:
 * f(x) = Relu(x) + (-alpha * Relu(-x))
 *
 * Practically, alpha is always a constant in the inference graph.
 * Therefore, we're looking for the structure:
 *
 * f(x) = Add(Relu(x), Mul(negative_alpha, Relu(Neg(x))))
 *
 * And generate the follow sub graph:
 * f(x) = Prelu(x, neg(negative_alpha))
 *
 * @param graph Graph, model graph object
 * @param weightMap NamedTensorsMap, the weight map for the executor.
 */
function rewritePrelu(graph, weightMap) {
    var _loop_1 = function (key) {
        var addNode = graph.nodes[key];
        if (addNode == null || addNode.op !== 'Add' && addNode.op !== 'AddV2' ||
            addNode.inputNames.length !== 2) {
            return "continue";
        }
        var reluNode = addNode.inputs.find(function (input) { return input.op === 'Relu'; });
        if (reluNode == null || reluNode.inputNames.length !== 1) {
            return "continue";
        }
        var mulOp = addNode.inputs.find(function (input) { return input.op === 'Mul'; });
        if (mulOp == null || mulOp.inputNames.length !== 2) {
            return "continue";
        }
        var negAlphaTensorNode = mulOp.inputs.find(function (input) { return input.op === 'Const'; });
        var reluNegInputNode = mulOp.inputs.find(function (input) { return input.op === 'Relu'; });
        if (negAlphaTensorNode == null || reluNegInputNode == null ||
            reluNegInputNode.inputNames.length !== 1) {
            return "continue";
        }
        // This detects a Neg op followed by a separated Relu op.
        var negInputNode = reluNegInputNode.inputs[0];
        if (negInputNode == null || negInputNode.op !== 'Neg' ||
            negInputNode.inputNames.length !== 1) {
            return "continue";
        }
        if (reluNode.inputNames[0] !== negInputNode.inputNames[0]) {
            return "continue";
        }
        var inputNode = reluNode.inputs[0];
        var outputNodes = addNode.children;
        // Construct a tensor for positive alpha (double negative).
        var alphaTensorName = negAlphaTensorNode.name + '_neg';
        var negNode = {
            name: alphaTensorName,
            inputNames: [],
            inputs: [],
            attrParams: {},
            category: 'graph',
            children: [],
            op: 'Const',
            inputParams: {},
            rawAttrs: {}
        };
        // Add the constant to weightMap
        weightMap[alphaTensorName] = [tfjs_core_1.neg(weightMap[negAlphaTensorNode.name][0])];
        graph.weights.push(negNode);
        // Construct the prelu node
        var preluNode = {
            name: addNode.name + '_Prelu',
            inputNames: [inputNode.name, negNode.name],
            inputs: [inputNode, negNode],
            attrParams: {},
            category: 'custom',
            children: outputNodes,
            op: 'Prelu',
            inputParams: {
                'x': { inputIndexStart: 0, type: 'tensor' },
                'alpha': { inputIndexStart: 1, type: 'tensor' }
            }
        };
        negNode.children.push(preluNode);
        // Clean up the children and inputs of input/output nodes of the subgraph.
        var mulIndex = negAlphaTensorNode.children.indexOf(mulOp);
        if (mulIndex > -1) {
            negAlphaTensorNode.children.splice(mulIndex, 1);
        }
        var reluIndex = inputNode.children.indexOf(reluNode);
        if (reluIndex > -1) {
            inputNode.children.splice(reluIndex, 1);
        }
        var negIndex = inputNode.children.indexOf(negInputNode);
        if (negIndex > -1) {
            inputNode.children.splice(negIndex, 1);
        }
        inputNode.children.push(preluNode);
        outputNodes.forEach(function (node) {
            var addIndex = node.inputNames.indexOf(addNode.name);
            if (addIndex > -1) {
                node.inputNames[addIndex] = preluNode.name;
                node.inputs[addIndex] = preluNode;
            }
        });
        // The prelu node should be an output node.
        if (outputNodes.length === 0) {
            var addIndex = graph.outputs.indexOf(addNode);
            if (addIndex > -1) {
                graph.outputs.splice(addIndex, 1);
            }
            graph.outputs.push(preluNode);
        }
        // remove the nodes for keras generated prelu subgraph.
        delete graph.nodes[addNode.name];
        delete graph.nodes[mulOp.name];
        delete graph.nodes[reluNode.name];
        delete graph.nodes[reluNegInputNode.name];
        delete graph.nodes[negInputNode.name];
        // add the newly generated nodes.
        graph.nodes[preluNode.name] = preluNode;
        graph.nodes[negNode.name] = negNode;
    };
    for (var key in graph.nodes) {
        _loop_1(key);
    }
}
exports.rewritePrelu = rewritePrelu;
//# sourceMappingURL=model_rewrite.js.map