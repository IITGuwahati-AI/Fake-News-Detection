{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "click_bait_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCfSiPU4sPgw",
        "colab_type": "code",
        "outputId": "8af20667-1435-4731-df47-d77701d44532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXelYQyqDAvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# from tf.keras.models import Sequential  # This does not work!\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding,Bidirectional,LSTM,GlobalMaxPool1D,Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.models import save_model\n",
        "import csv \n",
        "import pandas as pd \n",
        "from IPython.display import display, HTML\n",
        "import h5py\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMH6Q4gaDIoL",
        "colab_type": "code",
        "outputId": "73d4491d-5db6-42fa-b98b-09d47f4c3d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"drive/My Drive/Clickbait-Detection-master/clickbait17-train-170331\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Clickbait-Detection-master/clickbait17-train-170331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIx40QxxDG70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "truth_data = pd.read_json(\"truth.jsonl\", lines=True)\n",
        "truth_data.head()\n",
        "#testing data\n",
        "Ttruth_data = pd.read_json(\"Ttruth.jsonl\", lines=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKzO6hIMDSvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clickbait_data = pd.read_json(\"instances.jsonl\", lines=True)\n",
        "clickbait_data.head()\n",
        "#test data\n",
        "Tclickbait_data = pd.read_json(\"Tinstances.jsonl\", lines=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttiZBs8bDSye",
        "colab_type": "code",
        "outputId": "b39f36e5-a96d-444f-96c2-cf274cbf9031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "truth_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>truthJudgments</th>\n",
              "      <th>truthMean</th>\n",
              "      <th>id</th>\n",
              "      <th>truthClass</th>\n",
              "      <th>truthMedian</th>\n",
              "      <th>truthMode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>858464162594172928</td>\n",
              "      <td>clickbait</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.33333333330000003, 0.0, 0.33333333330000003...</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>858462320779026432</td>\n",
              "      <td>no-clickbait</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.33333333330000003, 0.6666666666000001, 1.0,...</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>858460992073863168</td>\n",
              "      <td>no-clickbait</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.0, 0.6666666666000001, 0.0, 0.3333333333000...</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>858459539296980992</td>\n",
              "      <td>no-clickbait</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>858455355948384256</td>\n",
              "      <td>no-clickbait</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      truthJudgments  ...  truthMode\n",
              "0                          [1.0, 1.0, 1.0, 1.0, 1.0]  ...   1.000000\n",
              "1  [0.33333333330000003, 0.0, 0.33333333330000003...  ...   0.000000\n",
              "2  [0.33333333330000003, 0.6666666666000001, 1.0,...  ...   0.000000\n",
              "3  [0.0, 0.6666666666000001, 0.0, 0.3333333333000...  ...   0.333333\n",
              "4                          [0.0, 0.0, 0.0, 0.0, 0.0]  ...   0.000000\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoB5oLlDDS05",
        "colab_type": "code",
        "outputId": "ad66b88a-b7f5-4bcc-bb3f-80b9df4c41ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "clickbait_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>postMedia</th>\n",
              "      <th>postText</th>\n",
              "      <th>id</th>\n",
              "      <th>targetCaptions</th>\n",
              "      <th>targetParagraphs</th>\n",
              "      <th>targetTitle</th>\n",
              "      <th>postTimestamp</th>\n",
              "      <th>targetKeywords</th>\n",
              "      <th>targetDescription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[]</td>\n",
              "      <td>[UK’s response to modern slavery leaving victi...</td>\n",
              "      <td>858462320779026432</td>\n",
              "      <td>[modern-slavery-rex.jpg]</td>\n",
              "      <td>[Thousands of modern slavery victims have not ...</td>\n",
              "      <td>‘Inexcusable’ failures in UK’s response to mod...</td>\n",
              "      <td>Sat Apr 29 23:25:41 +0000 2017</td>\n",
              "      <td>modern slavery, Department For Work And Pensio...</td>\n",
              "      <td>“Inexcusable” failures in the UK’s system for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[]</td>\n",
              "      <td>[this is good]</td>\n",
              "      <td>858421020331560960</td>\n",
              "      <td>[In this July 1, 2010 file photo, Dr. Charmain...</td>\n",
              "      <td>[President Donald Trump has appointed the pro-...</td>\n",
              "      <td>Donald Trump Appoints Pro-Life Advocate as Ass...</td>\n",
              "      <td>Sat Apr 29 20:41:34 +0000 2017</td>\n",
              "      <td>Americans United for Life, Dr. Charmaine Yoest...</td>\n",
              "      <td>President Donald Trump has appointed pro-life ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[]</td>\n",
              "      <td>[The \"forgotten\" Trump roast: Relive his bruta...</td>\n",
              "      <td>858368123753435136</td>\n",
              "      <td>[President Trump will not attend this year's W...</td>\n",
              "      <td>[When the White House correspondents’ dinner i...</td>\n",
              "      <td>The ‘forgotten’ Trump roast: Relive his brutal...</td>\n",
              "      <td>Sat Apr 29 17:11:23 +0000 2017</td>\n",
              "      <td>trump whcd, whcd, white house correspondents d...</td>\n",
              "      <td>President Trump won't be at this year's White ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[]</td>\n",
              "      <td>[Meet the happiest #dog in the world!]</td>\n",
              "      <td>858323428260139008</td>\n",
              "      <td>[Maru , Maru, Maru, Maru, Maru]</td>\n",
              "      <td>[Adorable is probably an understatement. This ...</td>\n",
              "      <td>Meet The Happiest Dog In The World, Maru The H...</td>\n",
              "      <td>Sat Apr 29 14:13:46 +0000 2017</td>\n",
              "      <td>Maru, husky, dogs, pandas, furball, instagram</td>\n",
              "      <td>The article is about Maru, a husky dog who has...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[]</td>\n",
              "      <td>[Tokyo's subway is shut down amid fears over a...</td>\n",
              "      <td>858283602626347008</td>\n",
              "      <td>[All nine lines of Tokyo's subway system were ...</td>\n",
              "      <td>[One of Tokyo's major subways systems says it ...</td>\n",
              "      <td>Tokyo's subway is shut down amid fears over an...</td>\n",
              "      <td>Sat Apr 29 11:35:31 +0000 2017</td>\n",
              "      <td>Tokyo,subway,shut,fears,North,Korean,attack</td>\n",
              "      <td>The temporary suspension, which lasted ten min...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  postMedia  ...                                  targetDescription\n",
              "0        []  ...  “Inexcusable” failures in the UK’s system for ...\n",
              "1        []  ...  President Donald Trump has appointed pro-life ...\n",
              "2        []  ...  President Trump won't be at this year's White ...\n",
              "3        []  ...  The article is about Maru, a husky dog who has...\n",
              "4        []  ...  The temporary suspension, which lasted ten min...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1j5dnHaDS3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.merge(clickbait_data,truth_data, on = 'id')\n",
        "data.head()\n",
        "Tdata = pd.merge(Tclickbait_data,Ttruth_data, on = 'id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIzOrzWUDS9Y",
        "colab_type": "code",
        "outputId": "c952f366-da7a-4177-d61f-40b44432b471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "data['postText']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [UK’s response to modern slavery leaving victi...\n",
              "1                                           [this is good]\n",
              "2        [The \"forgotten\" Trump roast: Relive his bruta...\n",
              "3                   [Meet the happiest #dog in the world!]\n",
              "4        [Tokyo's subway is shut down amid fears over a...\n",
              "                               ...                        \n",
              "19533    [Brazil soccer team and pilot's final intervie...\n",
              "19534                                     [😱😱😱😱😱😱😱😱😱😱😱😱😱😱]\n",
              "19535    [Frenchs Forest high school may have to make w...\n",
              "19536                                     [Oh Jeff… #bruh]\n",
              "19537    [Richard Sherman weighs in on Cam Newton’s str...\n",
              "Name: postText, Length: 19538, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKj17f6sET0l",
        "colab_type": "code",
        "outputId": "d24ce6e3-302b-4c87-cb49-bb1395400240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['postText'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UK’s response to modern slavery leaving victims destitute while abusers go free']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llUuPCD7ET3h",
        "colab_type": "code",
        "outputId": "1565c200-8909-459f-f487-6b6ab3c608aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data['targetDescription'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'“Inexcusable” failures in the UK’s system for dealing with modern slavery are\\xa0leaving victims reduced to destitution while their abusers go free because they are not adequately supported to testify against them, an alarming report has warned.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxzEdvmSET6O",
        "colab_type": "code",
        "outputId": "5e786782-b38e-48b4-8ce4-6dd0661bbe76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19538, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQkxgyRAPsaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS36v4CzP4uV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tdata.to_csv('Tdata.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBxLBINYrVHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('train1.csv')\n",
        "Tdata = pd.read_csv('test1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGSznonwF2oJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_text = data['concatenate']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0zxJdpmGGsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(thresh = 5):\n",
        "    count  = dict()\n",
        "    idx = 1\n",
        "    word_index = dict()\n",
        "    for txt in data_text:\n",
        "        words = process(txt)\n",
        "        for word in words:\n",
        "            if word in count.keys():\n",
        "                count[word] += 1\n",
        "            else:\n",
        "                count[word]  = 1\n",
        "    most_counts = [word for word in count.keys() if count[word]>=thresh]\n",
        "    for word in most_counts:\n",
        "        word_index[word] = idx\n",
        "        idx+=1\n",
        "    return word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihWCu7eYGGuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def process(txt):\n",
        "    out = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
        "    out = out.split()\n",
        "    out = [word.lower() for word in out]\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yail4GcGKVk",
        "colab_type": "code",
        "outputId": "cc24d61c-f89a-495d-8f98-7b06b3ba2b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_words = None\n",
        "word_index = tokenize()\n",
        "num_words = len(word_index)\n",
        "print('length of the dictionary ',len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of the dictionary  10935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV9hoTSSGKX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getMax(data):\n",
        "    max_tokens = 0 \n",
        "    for txt in data:\n",
        "        if max_tokens < len(txt.split()):\n",
        "            max_tokens = len(txt.split())\n",
        "    return max_tokens\n",
        "max_tokens = getMax(data['concatenate'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPA6n_0zr0e0",
        "colab_type": "code",
        "outputId": "4fcf5c1b-ed23-4581-8b82-c253ab684dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyTIHP8IGOkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sequences(data):\n",
        "    tokens = []\n",
        "    for txt in data:\n",
        "        words = process(txt)\n",
        "        seq = [0] * max_tokens\n",
        "        i = 0 \n",
        "        for word in words:\n",
        "            start = max_tokens-len(words)\n",
        "            if word.lower() in word_index.keys():\n",
        "                seq[i+start] = word_index[word]\n",
        "            i+=1\n",
        "        tokens.append(seq)        \n",
        "    return np.array(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bqh3zfHGPYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_tokens1 = create_sequences(data['concatenate'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Qp9CcJGPak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_tokens = create_sequences(Tdata['concatenate'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ0B4foqGPcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "embedding_size = 8\n",
        "model.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_tokens,\n",
        "                    name='layer_embedding'))\n",
        "\n",
        "model.add(GRU(units=16, name = \"gru_1\",return_sequences=True))\n",
        "model.add(GRU(units=8, name = \"gru_2\" ,return_sequences=True))\n",
        "model.add(GRU(units=4, name= \"gru_3\"))\n",
        "model.add(Dense(1, activation='sigmoid',name=\"dense_1\"))\n",
        "optimizer = Adam(lr=1e-3)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8youVj_GKen",
        "colab_type": "code",
        "outputId": "080b5c59-57cb-4981-97ee-04cb054b20da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "model.fit(x_train_tokens1, data['label'], validation_split=0.05, epochs=3, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 17665 samples, validate on 930 samples\n",
            "Epoch 1/3\n",
            "17665/17665 [==============================] - 335s 19ms/sample - loss: 0.5285 - acc: 0.7749 - val_loss: 0.4694 - val_acc: 0.7946\n",
            "Epoch 2/3\n",
            "17665/17665 [==============================] - 342s 19ms/sample - loss: 0.4238 - acc: 0.8182 - val_loss: 0.4650 - val_acc: 0.7839\n",
            "Epoch 3/3\n",
            "17665/17665 [==============================] - 339s 19ms/sample - loss: 0.3545 - acc: 0.8593 - val_loss: 0.4910 - val_acc: 0.7710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0e5dcb71d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWJl9EFMGKhN",
        "colab_type": "code",
        "outputId": "afa9ab36-a6a1-4f23-c52c-d76900386b27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print('\\n# Evaluate on test data')\n",
        "results = model.evaluate(x_test_tokens, Tdata['label'], batch_size=32)\n",
        "print('test loss, test acc:', results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "2267/2267 [==============================] - 9s 4ms/sample - loss: 0.6241 - acc: 0.7128\n",
            "test loss, test acc: [0.6240759046265981, 0.7128363]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELkHFEOEGKdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model(\n",
        "    model,\n",
        "    \"keras.h5\",\n",
        "    overwrite=True,\n",
        "    include_optimizer=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKzXUHKHGKai",
        "colab_type": "code",
        "outputId": "28454e6b-ada5-43a7-e87b-e7715dd47bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "txt = [\"[title] description\"]\n",
        "print(create_sequences(txt)[0])\n",
        "pred = model.predict(create_sequences(txt))\n",
        "print('\\n prediction for \\n',pred[:,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0  1622 10717]\n",
            "\n",
            " prediction for \n",
            " [0.42193747]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1LlbFmMxXlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_csv(file):\n",
        "    with open(file, 'w') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        for key in word_index.keys():\n",
        "            writer.writerow([key,word_index[key]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C9lonUFxaWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_csv('dict.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "374UQnWDGGxX",
        "colab_type": "code",
        "outputId": "b041d72a-f1d3-4576-b650-17f32d18128f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/ed/4c6aeff56a9c0816277931d5c2c6c2b3f98fcd8af1fcbbcc0d6a5a1c0403/tensorflowjs-1.6.0-py3-none-any.whl (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[?25hCollecting PyInquirer==1.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/4c/434b7c454010a284b49d6f1d446fe8dc5960415613d8c0225b9e2efb6724/PyInquirer-1.0.3.tar.gz\n",
            "Requirement already satisfied: tensorflow-hub==0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: h5py>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Collecting tensorflow-cpu==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/a9/d8e5118b4cc096633c04677809f0000519c43043b01311da02678349acf4/tensorflow_cpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (104.6MB)\n",
            "\u001b[K     |████████████████████████████████| 104.6MB 100kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.18.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting prompt_toolkit==1.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/3d/b25d35a9f0d381dd1c02d8e04b37c353caaaff4bc32150328eeebe4931f5/prompt_toolkit-1.0.14-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 48.2MB/s \n",
            "\u001b[?25hCollecting Pygments>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl (914kB)\n",
            "\u001b[K     |████████████████████████████████| 921kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2016.11.21 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2019.12.20)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.7.0->tensorflowjs) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.27.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.8.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.4.1)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 43.3MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (3.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.34.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt_toolkit==1.0.14->PyInquirer==1.0.3->tensorflowjs) (0.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.7.0->tensorflowjs) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (0.4.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.1.0)\n",
            "Building wheels for collected packages: PyInquirer, gast\n",
            "  Building wheel for PyInquirer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyInquirer: filename=PyInquirer-1.0.3-cp36-none-any.whl size=32851 sha256=2040bf38603a8d5cdc22fef88d3dfea71da13f082c743a1a4f9f4d1407faea28\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/6c/b1/3e4b0e8daf42a92883c7641c0ea8ffb62e0490ebed2faa55ad\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=9236303b34cf004b3dd5e31581b79a1c30e53cb2638e2fddb4d17fb4cfed3deb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built PyInquirer gast\n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit, Pygments, PyInquirer, tensorflow-estimator, tensorboard, gast, tensorflow-cpu, tensorflowjs\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed PyInquirer-1.0.3 Pygments-2.6.1 gast-0.2.2 prompt-toolkit-1.0.14 tensorboard-2.1.1 tensorflow-cpu-2.1.0 tensorflow-estimator-2.2.0rc0 tensorflowjs-1.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "prompt_toolkit",
                  "pygments",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPUUVdPIGG0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorflowjs_converter --input_format keras keras.h5 my_model_as_tfjs1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju4FyWIiET_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}