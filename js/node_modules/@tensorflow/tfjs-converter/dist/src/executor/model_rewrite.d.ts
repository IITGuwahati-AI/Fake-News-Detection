import { NamedTensorsMap } from '../data/types';
import { Graph } from '../operations/types';
/**
 * This graph rewrite rule tries to identify the PRelu structure generated by
 * tf.keras, and convert it to tfjs core prelu op.
 *
 * The formula of PReLU is:
 * f(x) = alpha * x for x < 0, f(x) = x for x >= 0.
 *
 * `x` is the input, and `alpha` is a trainable tensor which can be broadcasted
 * to the shape of `x`.
 *
 * There's no native PRelu op in TensorFlow, so tf.keras generates the following
 * structure which does the equivalent calculation:
 * f(x) = Relu(x) + (-alpha * Relu(-x))
 *
 * Practically, alpha is always a constant in the inference graph.
 * Therefore, we're looking for the structure:
 *
 * f(x) = Add(Relu(x), Mul(negative_alpha, Relu(Neg(x))))
 *
 * And generate the follow sub graph:
 * f(x) = Prelu(x, neg(negative_alpha))
 *
 * @param graph Graph, model graph object
 * @param weightMap NamedTensorsMap, the weight map for the executor.
 */
export declare function rewritePrelu(graph: Graph, weightMap: NamedTensorsMap): void;
